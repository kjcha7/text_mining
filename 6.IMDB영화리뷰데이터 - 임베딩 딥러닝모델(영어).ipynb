{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/StillWork/c9/blob/master/gg_67_%EB%8B%A8%EC%96%B4%EB%B2%A1%ED%84%B0_IMDB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Pa94qBD7EKp"
   },
   "source": [
    "# 단어 임베딩\n",
    "## 텍스트 분석에서 널리 사용됨\n",
    "- 자연어 처리, 문서 분류, 감성분석, 저작 식별, QA, 챗봇  \n",
    "\n",
    "## 토큰화\n",
    "- 단어, 글자, n-gram\n",
    "\n",
    "## 인코딩\n",
    "- 원-핫 인코딩: 단어집 크기의 벡터를 사용한다 (크기가 매우 크고 희소함)\n",
    "- 토큰 임베딩\n",
    "\n",
    "## 토큰 임베딩 \n",
    "- 단어 임베딩\n",
    "- 글자 임베딩\n",
    "- n-gram 임베딩\n",
    "\n",
    "### 원-핫 인코딩은 문장의 구조 정보를 잃어버린다\n",
    "\n",
    "## 단어 벡터\n",
    "- 희소 베터대신 밀집 벡터를 사용\n",
    "- id를 구분하는 용도가 아니라 공간상의 점을 매핑 (거리와 벡터를 제공)\n",
    "\n",
    "### 생성 방법\n",
    "- 특정한 문제를 해결하는 과정에서 (모델을 만드는 과정에서) 부수적으로 단어벡터를 만들 수 있다. 단어간의 의미를 반영한다. 의미가 가까우면 거리도 가깝게. \n",
    "- 다른 곳에서 만든 단어벡터를 로드하여 사용\n",
    "\n",
    "\n",
    "### Embedding 계층을 사용하여 쉽게 만들 수 있다\n",
    "- 정수 인덱스를 벡터로 매핑하는 딕셔너리 구조 (인덱스 크기, 벡터 크기)\n",
    "- 학습 시키는 데이터에 따라 다른 임베딩이 만들어진다.\n",
    "\n",
    "### IMDB 영화 리뷰 데이터를 사용한 임베딩 예제\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y8hRGgn85LtI",
    "outputId": "bb61d2a7-3a71-434a-c215-92e84caaaa02"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten, Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "import os, os.path\n",
    "import zipfile\n",
    "from keras.datasets import imdb\n",
    "from keras import preprocessing\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "9Tu8xAnn_nGl",
    "outputId": "2a2fd72d-264f-4685-ae4f-f47256b28c2f"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 20\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gXuG1WkZeq6L",
    "outputId": "6e387049-c980-47d2-e07b-cd7f5c6f1e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J4w1ON5Z7SvH",
    "outputId": "4f2b4c0a-f89f-4794-aea4-f0183774dbe3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "blQtfwq4e3NP"
   },
   "outputs": [],
   "source": [
    "x_train=preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test=preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1_WuOAkafd8M",
    "outputId": "0bef4697-1c73-47b8-df5a-3fb82b9ad10f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "id": "KD5dWA__fqO1",
    "outputId": "85b24810-b6e5-4d3c-8897-f0e41a04bb93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 8)             80000     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 161       \n",
      "=================================================================\n",
      "Total params: 80,161\n",
      "Trainable params: 80,161\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 8, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "colab_type": "code",
    "id": "LiMsP3C96RRg",
    "outputId": "ccb7f637-6f80-4bde-c774-595874591770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 2s 93us/step - loss: 0.6759 - acc: 0.6050 - val_loss: 0.6398 - val_acc: 0.6814\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 2s 80us/step - loss: 0.5657 - acc: 0.7427 - val_loss: 0.5467 - val_acc: 0.7206\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 2s 82us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 2s 81us/step - loss: 0.4263 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 2s 79us/step - loss: 0.3930 - acc: 0.8258 - val_loss: 0.4981 - val_acc: 0.7538\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 2s 80us/step - loss: 0.3668 - acc: 0.8395 - val_loss: 0.5014 - val_acc: 0.7530\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 2s 82us/step - loss: 0.3435 - acc: 0.8533 - val_loss: 0.5052 - val_acc: 0.7520\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 2s 80us/step - loss: 0.3223 - acc: 0.8657 - val_loss: 0.5132 - val_acc: 0.7486\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 2s 78us/step - loss: 0.3022 - acc: 0.8766 - val_loss: 0.5213 - val_acc: 0.7490\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 2s 77us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5303 - val_acc: 0.7466\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3NYmQslQ_Edk"
   },
   "source": [
    "## 위의 결과는 20개의 단어만 고려한 것임\n",
    "### 성능이 75% 정도 됨\n",
    "- 각 단어를 독립적으로 다루었으며, 문장의 구성 정보를 고려하지 않음\n",
    "- 문장의 구조 정보를 고려하려면 임베딩 층 위에 합성곱이나 순환신경망 층을 추가한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HI9yPocrBjq-"
   },
   "source": [
    "## 사전훈련된 임베딩 사용\n",
    "- 사전 학습된 합성곱 신경망을 사용하는 것과 같이 임베딩도 사전훈련된 값을 사용할 수 있다.\n",
    "### 이 예에서는 성능 개선이 없다 \n",
    "\n",
    "## 사전 학습 임베딩\n",
    "- 2013년 구글의 Word2Vect\n",
    "- 2014년 스탠포드의 GloVe: 단어의 동시출현 통계를 분석 \n",
    "\n",
    "## 원본 IMDB를 사용하여 (토큰화 되기 전의) 텍스트 처리부터 작업을 하겠다\n",
    "- 훈련 데이터가 부족할 때 유용하다\n",
    "- 훈련 데이터가 충분하면 직접 학습하는 것이 낫다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "kfa_ykLYDLdv",
    "outputId": "4264fbf9-3436-47d1-ee9d-8d3505c56ec4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-03 23:50:15--  http://mng.bz/0tIo\n",
      "Resolving mng.bz (mng.bz)... 35.166.24.88\n",
      "Connecting to mng.bz (mng.bz)|35.166.24.88|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://mng.bz/0tIo [following]\n",
      "--2019-05-03 23:50:16--  https://mng.bz/0tIo\n",
      "Connecting to mng.bz (mng.bz)|35.166.24.88|:443... connected.\n",
      "WARNING: cannot verify mng.bz's certificate, issued by ‘CN=Go Daddy Secure Certificate Authority - G2,OU=http://certs.godaddy.com/repository/,O=GoDaddy.com\\\\, Inc.,L=Scottsdale,ST=Arizona,C=US’:\n",
      "  Unable to locally verify the issuer's authority.\n",
      "HTTP request sent, awaiting response... 301 \n",
      "Location: http://s3.amazonaws.com/text-datasets/aclImdb.zip [following]\n",
      "--2019-05-03 23:50:17--  http://s3.amazonaws.com/text-datasets/aclImdb.zip\n",
      "Resolving s3.amazonaws.com (s3.amazonaws.com)... 54.231.98.123\n",
      "Connecting to s3.amazonaws.com (s3.amazonaws.com)|54.231.98.123|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 60711700 (58M) [application/zip]\n",
      "Saving to: ‘./data/IMDB.zip’\n",
      "\n",
      "./data/IMDB.zip     100%[===================>]  57.90M  2.22MB/s    in 47s     \n",
      "\n",
      "2019-05-03 23:51:04 (1.24 MB/s) - ‘./data/IMDB.zip’ saved [60711700/60711700]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "if not os.path.exists(\"./data/IMDB.zip\"):\n",
    "  !wget --no-check-certificate http://mng.bz/0tIo \\\n",
    "  -O ./data/IMDB.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "V_sU7g6lDWXS",
    "outputId": "15f4f9ac-c535-411e-a9ca-5ce3168fe2be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aclImdb_v1.tar.gz  MoviesampleSubmission.csv  ratings_train.txt\r\n",
      "alice_mask.png\t   movie_test.tsv\t      sampleSubmission.csv\r\n",
      "alice.txt\t   movie_train.tsv\t      stormtrooper_mask.png\r\n",
      "a_new_hope.txt\t   news\t\t\t      test.tsv\r\n",
      "BEXX0004.txt\t   NewsResult.xlsx\t      toji\r\n",
      "heart.jpg\t   News.xlsx\t\t      train.tsv\r\n",
      "IMDB.zip\t   ratings_test.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lJDWfvuSDk8f"
   },
   "outputs": [],
   "source": [
    "local_zip = './data/IMDB.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./data')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-RQIqJIE9h0"
   },
   "outputs": [],
   "source": [
    "imdb_dir='./data/aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train')\n",
    "labels = []\n",
    "texts = []\n",
    "\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type)\n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':\n",
    "            f = open(os.path.join(dir_name, fname))\n",
    "            texts.append(f.read())\n",
    "            f.close()\n",
    "            if label_type == 'neg':\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "l_h-KUjnFXg0",
    "outputId": "2f26938e-d92a-4c41-c441-fdd28f396506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "\n",
    "maxlen = 100  # We will cut reviews after 100 words\n",
    "training_samples = 10000  # We will be training on 200 samples\n",
    "validation_samples = 10000  # We will be validating on 10000 samples\n",
    "max_words = 10000  # We will only consider the top 10,000 words in the dataset\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=maxlen)\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# Split the data into a training set and a validation set\n",
    "# But first, shuffle the data, since we started from data\n",
    "# where sample are ordered (all negative first, then all positive).\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "x_train = data[:training_samples]\n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples]\n",
    "y_val = labels[training_samples: training_samples + validation_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "pkNWHejPHNP4",
    "outputId": "89c8ff15-2dfd-459f-c140-c87f134837b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25000, 100), (25000,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pad_sequences(sequences, maxlen=maxlen)\n",
    "labels=np.asarray(labels)\n",
    "data.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "id": "3ai7oUGbcaI2",
    "outputId": "1994c4bb-4601-4a75-aee9-d6e6d093daa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-05-03 23:51:28--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2019-05-03 23:51:29--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘./data/glove6B.zip’\n",
      "\n",
      "./data/glove6B.zip  100%[===================>] 822.24M  2.06MB/s    in 8m 17s  \n",
      "\n",
      "2019-05-03 23:59:46 (1.66 MB/s) - ‘./data/glove6B.zip’ saved [862182613/862182613]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./data\"):\n",
    "    os.makedirs(\"./data\")\n",
    "if not os.path.exists(\"./data/glove6B.zip\"):\n",
    "  !wget --no-check-certificate http://nlp.stanford.edu/data/glove.6B.zip \\\n",
    "  -O ./data/glove6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "colab_type": "code",
    "id": "W9RvL73zd3I5",
    "outputId": "d03fd66c-2c3b-4875-d838-8f2551914684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1045976\r\n",
      "drwxrwxr-x 6 kjchar kjchar      4096 May  3 23:51 .\r\n",
      "drwxrwxr-x 8 kjchar kjchar      4096 May  3 23:59 ..\r\n",
      "drwxrwxr-x 4 kjchar kjchar      4096 May  3 23:51 aclImdb\r\n",
      "-rw-rw-r-- 1 kjchar kjchar  84125825 Jun 16  2018 aclImdb_v1.tar.gz\r\n",
      "-rw-rw-r-- 1 kjchar kjchar      7339 May  3 23:41 alice_mask.png\r\n",
      "-rw-rw-r-- 1 kjchar kjchar    148570 May  3 23:40 alice.txt\r\n",
      "-rw-rw-r-- 1 kjchar kjchar    326362 May  3 23:45 a_new_hope.txt\r\n",
      "-rw-rw-r-- 1 kjchar kjchar    913902 Jan  8 16:24 BEXX0004.txt\r\n",
      "-rw-rw-r-- 1 kjchar kjchar 862182613 Oct 25  2015 glove6B.zip\r\n",
      "-rw-rw-r-- 1 kjchar kjchar     78856 May  3 23:45 heart.jpg\r\n",
      "-rw-rw-r-- 1 kjchar kjchar  60711700 Nov 18  2017 IMDB.zip\r\n",
      "drwxr-xr-x 2 kjchar kjchar      4096 May  3 23:31 .ipynb_checkpoints\r\n",
      "drwxrwxr-x 3 kjchar kjchar      4096 May  3 23:51 __MACOSX\r\n",
      "-rw-rw-r-- 1 kjchar kjchar    596647 Jan  8 15:37 MoviesampleSubmission.csv\r\n",
      "-rw-rw-r-- 1 kjchar kjchar   3367149 Jul 13  2018 movie_test.tsv\r\n",
      "-rw-rw-r-- 1 kjchar kjchar   8481022 Jul 13  2018 movie_train.tsv\r\n",
      "drwxrwxr-x 2 kjchar kjchar      4096 Jul 10  2018 news\r\n",
      "-rw-rw-r-- 1 kjchar kjchar   9255436 May  3 23:25 NewsResult.xlsx\r\n",
      "-rw-rw-r-- 1 kjchar kjchar   7897331 May  3 23:31 News.xlsx\r\n",
      "-rw-rw-r-- 1 kjchar kjchar   4893335 Jul 13  2018 ratings_test.txt\r\n",
      "-rw-rw-r-- 1 kjchar kjchar  14628807 Jul 13  2018 ratings_train.txt\r\n",
      "-rw-rw-r-- 1 kjchar kjchar    596647 Jul 13  2018 sampleSubmission.csv\r\n",
      "-rw-rw-r-- 1 kjchar kjchar     12601 May  3 23:45 stormtrooper_mask.png\r\n",
      "-rw-rw-r-- 1 kjchar kjchar   3433442 Jun 16  2018 test.tsv\r\n",
      "-rw-rw-r-- 1 kjchar kjchar    709140 May  3 23:48 toji\r\n",
      "-rw-rw-r-- 1 kjchar kjchar   8637083 Jun 16  2018 train.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -al data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XxUrM2XeIRd"
   },
   "outputs": [],
   "source": [
    "local_zip = './data/glove6B.zip'\n",
    "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
    "zip_ref.extractall('./data')\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "901R6N6GgNgC",
    "outputId": "2e65e677-59af-4e19-e012-d8a0496d514b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_dir='data'\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'))\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwiTxvnPeKss"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if i < max_words:\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "WbAnitc3h6KE",
    "outputId": "b66e2cdd-f64a-4858-9b68-58385d7e4aa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                320032    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,320,065\n",
      "Trainable params: 1,320,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZuZOjU2Fnw1"
   },
   "outputs": [],
   "source": [
    "model.layers[0].set_weights([embedding_matrix])\n",
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "7JSaipmSGPNO",
    "outputId": "72ec06b1-b7ea-4886-da24-4f760547760c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 3s 258us/step - loss: 0.7038 - acc: 0.5559 - val_loss: 0.6551 - val_acc: 0.6479\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 2s 223us/step - loss: 0.6141 - acc: 0.6709 - val_loss: 0.6168 - val_acc: 0.6594\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 0.5155 - acc: 0.7505 - val_loss: 0.5664 - val_acc: 0.7090\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 2s 231us/step - loss: 0.4409 - acc: 0.7899 - val_loss: 0.5868 - val_acc: 0.7021\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 2s 226us/step - loss: 0.3791 - acc: 0.8293 - val_loss: 0.6054 - val_acc: 0.7017\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 2s 235us/step - loss: 0.3307 - acc: 0.8574 - val_loss: 0.6727 - val_acc: 0.6978\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 2s 238us/step - loss: 0.2847 - acc: 0.8823 - val_loss: 0.7464 - val_acc: 0.6765\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 2s 224us/step - loss: 0.2323 - acc: 0.9065 - val_loss: 0.7984 - val_acc: 0.6765\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 0.1895 - acc: 0.9258 - val_loss: 0.7874 - val_acc: 0.6841\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 2s 224us/step - loss: 0.1498 - acc: 0.9455 - val_loss: 0.9026 - val_acc: 0.6822\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=32,\n",
    "                    validation_data=(x_val, y_val))\n",
    "model.save_weights('pre_trained_glove_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 723
    },
    "colab_type": "code",
    "id": "uCLVsbOBG5hU",
    "outputId": "e03b109b-dfef-41b6-c303-5b8ce2d13a29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Training and validation loss')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo')\n",
    "plt.plot(epochs, val_acc, 'b')\n",
    "plt.title('Training and validation accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo')\n",
    "plt.plot(epochs, val_loss, 'b')\n",
    "plt.title('Training and validation loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9e5XPjL7HIdI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "gg_67_단어벡터_IMDB.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
